# AI-Client
# Copyright (C) 2025 SharkyMew
# Licensed under the GNU AGPL v3 or later.

import time
import random
import threading
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
import torch

# ================== é…ç½®åŒº ==================
MODEL_NAME = "./MewyShark" # æ›´æ¢ä¸ºæœ¬åœ°æ¨¡å‹è·¯å¾„æˆ–Hugging Faceçš„æ¨¡å‹è·¯å¾„
MAX_HISTORY_TOKENS = 8192  # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆtoken æ•°ï¼‰
MAX_NEW_TOKENS = 512       # å•æ¬¡å›å¤æœ€å¤§é•¿åº¦
THINKING_MIN = 0         # æœ€å°â€œæ€è€ƒâ€æ—¶é—´ï¼ˆç§’ï¼‰ã€Œè°ƒè¯•ç”¨ï¼Œå¼ƒç”¨ã€
THINKING_MAX = 0        # æœ€å¤§â€œæ€è€ƒâ€æ—¶é—´ï¼ˆç§’ï¼‰ ã€Œè°ƒè¯•ç”¨ï¼Œå¼ƒç”¨ã€

# ================== åˆå§‹åŒ–æ¨¡å‹ ==================
print(f"æ­£åœ¨åŠ è½½ï¼š{MODEL_NAME} ...")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

# æ£€æŸ¥è®¾å¤‡ï¼Œæ³¨ï¼šä»…æ”¯æŒmpsæˆ–cpuï¼Œcudaè¯·åœ¨ä¸‹æ–¹ä¿®æ”¹ï¼Œmpsä»…æ”¯æŒApple Mç³»åˆ—SoCï¼ŒIntel Macä»…å¯ä½¿ç”¨cpu
device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    trust_remote_code=True,
    low_cpu_mem_usage=True
).to(device) # cudaè¯·åœ¨æ­¤å¤„å°†deviceæ›´æ¢ä¸º"cuda"ï¼ˆå¸¦å¼•å·ï¼‰

# ================== ç³»ç»Ÿæç¤ºè¯ï¼ˆåŸºç¡€éƒ¨åˆ†ï¼‰==================
system_prompt_base = (
    '''ä½ æ˜¯ä¸€ä¸ªAssistantï¼Œä¼šå¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ï¼Œå½“å‰æ—¶é—´æ˜¯ï¼š'''
)

# åˆå§‹åŒ–å¯¹è¯å†å²
history = []

def update_system_message():
    """åŠ¨æ€æ›´æ–°ç³»ç»Ÿæ¶ˆæ¯ä¸­çš„æ—¶é—´"""
    current_time = time.asctime()
    return {
        "role": "system",
        "content": system_prompt_base + current_time
    }

# åˆå§‹åŠ è½½
history = [update_system_message()]

print("=== æ¬¢è¿ä¸ Assistant èŠå¤©ï¼è¾“å…¥å†…å®¹å’Œå®ƒèŠå¤©å§ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰ ===")

# ================== ä¸Šä¸‹æ–‡è£å‰ªå‡½æ•° ==================
def trim_history(history, tokenizer, max_tokens=MAX_HISTORY_TOKENS):
    """ä»å¤´å¼€å§‹åˆ é™¤æ—§å¯¹è¯ï¼Œç›´åˆ°æ€»é•¿åº¦å°äº max_tokens"""
    while True:
        try:
            total_tokens = len(tokenizer.apply_chat_template(history, tokenize=True))
            if total_tokens <= max_tokens or len(history) <= 2:
                break
            # åˆ é™¤æœ€æ—©çš„ user/assistant å¯¹è¯ï¼ˆè·³è¿‡ systemï¼‰
            if len(history) > 2:
                history.pop(1)
                if len(history) > 2:
                    history.pop(1)
        except:
            break
    return history

# ================== ä¸»å¾ªç¯ ==================
while True:
    user_input = input("\nä½ ï¼š").strip()
    if user_input.lower() in ["exit", "quit", "é€€å‡º"]:
        print("Assistantï¼šå“¼ï¼Œä¸èŠå•¦ï¼Œä¸‹æ¬¡è§ï¼ğŸ‘‹")
        break

    if not user_input:
        print("Assistantï¼šâ€¦â€¦æ‚é±¼ï¼Œä½ è¿è¯éƒ½è¯´ä¸å‡ºæ¥å—ï¼Ÿ")
        continue

    # æ›´æ–°ç³»ç»Ÿæ—¶é—´
    history[0] = update_system_message()
    history.append({"role": "user", "content": user_input})

    # è£å‰ªå†å²
    history = trim_history(history, tokenizer, MAX_HISTORY_TOKENS)

    # æ„é€  prompt
    try:
        prompt = tokenizer.apply_chat_template(history, tokenize=False, add_generation_prompt=True)
    except Exception as e:
        print(f"Assistantï¼šå•§ï¼Œæç¤ºè¯å‡ºé—®é¢˜äº†ï¼š{e}")
        continue

    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    # åˆ›å»º streamer
    streamer = TextIteratorStreamer(
        tokenizer,
        skip_prompt=True,
        skip_special_tokens=True
    )

    # ç”Ÿæˆå‚æ•°ï¼Œå¯æ ¹æ®éœ€è¦ä¿®æ”¹
    gen_kwargs = {
        **inputs,
        "max_new_tokens": MAX_NEW_TOKENS,
        "do_sample": True,
        "temperature": 0.6,
        "top_p": 0.8,
        "repetition_penalty": 1.3,
        "streamer": streamer,
        "pad_token_id": tokenizer.eos_token_id
    }

    # æ¨¡æ‹Ÿâ€œæ€è€ƒä¸­â€å»¶è¿Ÿï¼ˆè°ƒè¯•ç”¨ï¼Œå¼ƒç”¨ï¼‰
    thinking_delay = random.uniform(THINKING_MIN, THINKING_MAX)
    print("\nAssistantï¼š", end="", flush=True)
    # time.sleep(thinking_delay)

    # å¯åŠ¨ç”Ÿæˆçº¿ç¨‹
    thread = threading.Thread(target=model.generate, kwargs=gen_kwargs)
    thread.start()

    # ç­‰å¾… streamer å‡†å¤‡å¥½ç¬¬ä¸€ä¸ª tokenï¼ˆå¼ƒç”¨ï¼‰
    # while not streamer.next_tokens_are_ready and thread.is_alive():
    #     time.sleep(0.01)

    # å®æ—¶è¾“å‡º
    reply_text = ""
    try:
        for new_text in streamer:
            print(new_text, end="", flush=True)
            reply_text += new_text
        print()  # æ¢è¡Œ
    except Exception as e:
        print(f"\nAssistantï¼šå•Šâ€¦â€¦å¤´å¥½æ™•ï¼Œåˆšæ‰æ–­ç‰‡äº†ï¼š{e}")

    # æ·»åŠ åˆ°å†å²
    if reply_text.strip():
        history.append({"role": "assistant", "content": reply_text.strip()})
    else:
        print("Assistantï¼šâ€¦â€¦ä»Šå¤©ä¸æƒ³ç†ä½ ã€‚")
        history.append({"role": "assistant", "content": "â€¦â€¦ä»Šå¤©ä¸æƒ³ç†ä½ ã€‚"})